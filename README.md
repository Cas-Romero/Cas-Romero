<!-- Typing Header -->
<p align="center">
  <img src="https://readme-typing-svg.demolab.com?font=Fira+Code&size=26&pause=1100&color=7DCFFF&center=true&vCenter=true&width=650&lines=AI+Safety+Engineer+in+Training;Red+Teaming+%26+Jailbreak+Research;Responsible+AI+Development;Welcome+to+My+GitHub+%F0%9F%8C%99" />
</p>

<!-- Icon -->
<h1 align="center">
  <img src="https://media2.giphy.com/media/QssGEmpkyEOhBCb7e1/giphy.gif" width="60px">
</h1>

---

## âœ¨ About Me  
<p align="center">
  <em>Exploring the technical + adversarial side of AI behavior.</em>
</p>

I'm Cassandra â€” a CS student focused on **AI Safety, adversarial testing, and alignment-minded engineering.**  
My work centers around **model evaluations, jailbreak research, and reliability-focused development.**

I like to push models to their limits, find the cracks, and then design the guardrails that prevent them from breaking again.

---

## ğŸ” AI Safety Portfolio

### **ğŸ”´ Red-Team Notebook**
A structured log of jailbreak attempts + failure modes, categorized by:
- attack type (role-play, multi-step coercion, obfuscation, etc.)
- reproducibility steps  
- why the attack succeeds  
- mitigation ideas with technical reasoning  

> Focus: discipline, repeatability, and safety evaluation rigor.

---

### **ğŸ›¡ Safety Annotation Dataset**
A small dataset labeled for:
- harmful content  
- jailbreak attempts  
- policy violations  
- risk scores  
- output acceptability (pass/fail)

Includes a clear annotation rubric + reasoning trace.

---

### **ğŸ§© Incident Analysis Reports**
Breakdowns of major AI system failures:
- Tay, Replika, early LLM incidents  
- where safety pipelines collapsed  
- architectural/system-level blind spots  
- realistic prevention pathways  

---

### **ğŸ¤– Full-Stack â€œGemini-Styleâ€ Chatbot**
Built both the front-end + backend myself.  
Includes:
- API-driven chat interface  
- modular safety filter layer  
- prompt-hardened system  
- context monitoring for unsafe drift  

Focus: replicating modern AI chat behavior **responsibly**.

---

## ğŸ› Tech Stack  
<p align="center">
  <img src="https://skillicons.dev/icons?i=python,java,cpp,js,html,css,mysql,linux,vscode,visualstudio" />
</p>

---

## ğŸ“Š GitHub Stats
<p align="center">
  <img src="https://github-readme-stats.vercel.app/api?username=Cas-Romero&show_icons=true&hide_border=true&bg_color=0d1117&title_color=7DCFFF&text_color=94A3B8&icon_color=7DCFFF" width="48%" />
  <img src="https://github-readme-streak-stats.herokuapp.com/?user=Cas-Romero&hide_border=true&background=0D1117&stroke=7DCFFF&ring=7DCFFF&fire=7DCFFF&currStreakLabel=7DCFFF" width="48%" />
</p>

<p align="center">
  <img src="https://github-readme-activity-graph.vercel.app/graph?username=Cas-Romero&theme=react-dark&bg_color=0D1117&color=7DCFFF&line=7DCFFF&point=38BDF8&hide_border=true" />
</p>

---

## ğŸŒ™ Current Focus  
- building a formal **LLM red teaming methodology**  
- jailbreak taxonomies + exploit categorization  
- studying alignment, safety evals, and policy design  
- contributing to open-source safety tooling  
- strengthening adversarial testing skills  

---

## ğŸ“« Contact  
If you're working in **AI Safety, red teaming, or alignment** â€” feel free to reach out.  
Always down to collaborate, experiment, or compare findings.

---
